{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ6rWPBBnkBz",
        "outputId": "cf62e8fc-69ac-46fb-bc8f-28dde10654e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-fnx3z0dt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-fnx3z0dt\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4295 sha256=045203127f43cc3057952d6318fb84657670fae528f2d5f2af05ea10bbe3d76a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zz28fpsw/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <time.h>\n",
        "#define BLOCK_SIZE 32\n",
        "#define RADIUS 1\n",
        "\n",
        "__host__ void init_vector(int** v, int n, int val);\n",
        "__host__ void operate(int* A, int* B, int n);\n",
        "__host__ bool check_result(int* A, int* B, int n);\n",
        "__global__ void reduce(int* A, int* B, int n);\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    srand(time(NULL));\n",
        "\n",
        "    int n = (1 << 4),\n",
        "        *A = nullptr,\n",
        "        *B = nullptr;\n",
        "\n",
        "    init_vector(&A, n + 2, 0);\n",
        "    init_vector(&B, n, 0);\n",
        "\n",
        "    operate(A, B, n);\n",
        "\n",
        "    free(B);\n",
        "    free(A);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "__host__ void init_vector(int** v, int n, int val)\n",
        "{\n",
        "    *v = (int*)malloc(sizeof(int) * n);\n",
        "    for(int i = 0; i < n; (*v)[i++]=i);\n",
        "    for(int i = 0; i < n; i++)\n",
        "      printf(\"%d \", (*v)[i]);\n",
        "    printf(\"\\n\");\n",
        "}\n",
        "\n",
        "__host__ void operate(int* A, int* B, int n)\n",
        "{\n",
        "    int* dev_A, *dev_B;\n",
        "\n",
        "    size_t count_A = sizeof(int) * (n + 2), count_B = sizeof(int) * n;\n",
        "    cudaError_t err;\n",
        "\n",
        "    err = cudaMalloc(&dev_A, count_A);\n",
        "    if(err)\n",
        "      printf(\"1A %s\\n\", cudaGetErrorString(err));\n",
        "\n",
        "    err = cudaMalloc(&dev_B, count_B);\n",
        "    if(err)\n",
        "      printf(\"1B %s\\n\", cudaGetErrorString(err));\n",
        "\n",
        "    err = cudaMemcpy(dev_A, A, count_A, cudaMemcpyHostToDevice);\n",
        "    if(err)\n",
        "      printf(\"2 %s\\n\", cudaGetErrorString(err));\n",
        "\n",
        "    reduce<<<n / BLOCK_SIZE + 1, BLOCK_SIZE>>>(dev_A, dev_B, 18);\n",
        "\n",
        "    err = cudaMemcpy(B, dev_B, count_B, cudaMemcpyDeviceToHost);\n",
        "    if(err)\n",
        "      printf(\"3 %s\\n\", cudaGetErrorString(err));\n",
        "\n",
        "    for(int i = 0; i < n; i++)\n",
        "      printf(\"%d \", B[i]);\n",
        "\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_A);\n",
        "}\n",
        "\n",
        "__global__ void reduce(int* A, int* B, int n)\n",
        "{\n",
        "    __shared__ int sh_in[BLOCK_SIZE];\n",
        "    int gidx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int lidx = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Load data into shared memory\n",
        "    if (gidx < n) {\n",
        "        sh_in[lidx] = A[gidx];\n",
        "        if (threadIdx.x < RADIUS) {\n",
        "\n",
        "            sh_in[lidx - RADIUS] = A[gidx];\n",
        "            sh_in[lidx + BLOCK_SIZE] = A[gidx + BLOCK_SIZE];\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Apply stencil\n",
        "    int sum = 0;\n",
        "    if (gidx < n) {\n",
        "            sum = sh_in[lidx]+sh_in[lidx + 1]+sh_in[lidx + 2];\n",
        "        B[gidx] = sum;\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cHVzbPYnp22",
        "outputId": "acf40ada-6d4f-45a0-cdb1-a2021afd9810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \n",
            "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n",
            "3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dJDXfp8wpn7C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}