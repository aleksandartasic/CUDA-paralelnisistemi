{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTasENs0SUkO",
        "outputId": "6be9b315-1a89-47e7-a8c2-359e26f74c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suma reda 0: 0\n",
            "Suma reda 1: 16\n",
            "Suma reda 2: 32\n",
            "Suma reda 3: 48\n",
            "Suma reda 4: 64\n",
            "Suma reda 5: 80\n",
            "Suma reda 6: 96\n",
            "Suma reda 7: 112\n",
            "Suma reda 8: 128\n",
            "Suma reda 9: 144\n",
            "Suma reda 10: 160\n",
            "Suma reda 11: 176\n",
            "Suma reda 12: 192\n",
            "Suma reda 13: 208\n",
            "Suma reda 14: 224\n",
            "Suma reda 15: 240\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 16 // rows\n",
        "#define M 16 // columns\n",
        "#define BLOCK_SIZE 4 // threads per block\n",
        "\n",
        "__global__ void sumMatrixRows(int* matrix, int* result) {\n",
        "    __shared__ int parcijalneSume[BLOCK_SIZE];\n",
        "\n",
        "    // Initialize partial sum for this thread\n",
        "    int parcijalSum = 0;\n",
        "    int tid=threadIdx.x;\n",
        "\n",
        "    parcijalSum += matrix[ blockIdx.x *2*  blockDim.x + threadIdx.x] + matrix [ blockIdx.x * 2 *  blockDim.x + threadIdx.x + BLOCK_SIZE];\n",
        "\n",
        "    parcijalneSume[threadIdx.x] = parcijalSum;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {\n",
        "        if (threadIdx.x < stride) {\n",
        "            parcijalneSume[threadIdx.x] += parcijalneSume[threadIdx.x + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        result[ (blockIdx.x) ] += parcijalneSume[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int matrix[N][M];\n",
        "    int result[N];\n",
        "    int* d_matrix;\n",
        "    int* d_result;\n",
        "\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < M; j++) {\n",
        "            matrix[i][j] = i;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaMalloc((void**)&d_matrix, N * M * sizeof(int));\n",
        "    cudaMalloc((void**)&d_result, N * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_matrix, matrix, N * M * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    sumMatrixRows<<<N, N/2>>>(d_matrix, d_result);\n",
        "\n",
        "    cudaMemcpy(result, d_result, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaFree(d_matrix);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        printf(\"Suma reda %d: %d\\n\", i, result[i]);\n",
        "    }\n",
        "    return 0;\n",
        "}///POSLEDNJE MODIFIKACIJE NE RADE 28.8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnwTghZ64hQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d923bf16-f4b5-474b-d3d6-56bbb748dfed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 16\n",
            "Suma po blokovima JE 15\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 256 // rows\n",
        "#define M 256 // columns\n",
        "#define BLOCK_SIZE 8 // threads per block\n",
        "\n",
        "__global__ void sumDiagonalOfMatrix(int* matrix, int* result) {\n",
        "    __shared__ int parcijalneSume[BLOCK_SIZE];\n",
        "\n",
        "    // Initialize partial sum for this thread\n",
        "    int parcijalSum = 0;\n",
        "    int tid=threadIdx.x;\n",
        "\n",
        "    parcijalSum += matrix[ ( blockIdx.x * 2 *  blockDim.x + threadIdx.x ) * N + blockIdx.x * 2 *  blockDim.x + threadIdx.x ] +\n",
        "    matrix [ ( blockIdx.x * 2 *  blockDim.x + threadIdx.x + BLOCK_SIZE ) * N + blockIdx.x * 2 *  blockDim.x + threadIdx.x + BLOCK_SIZE + 1 ];\n",
        "\n",
        "    parcijalneSume[threadIdx.x] = parcijalSum;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {\n",
        "        if (threadIdx.x < stride) {\n",
        "            parcijalneSume[threadIdx.x] += parcijalneSume[threadIdx.x + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        result[blockIdx.x] = parcijalneSume[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int matrix[N][M];\n",
        "    int result[N];\n",
        "    int* d_matrix;\n",
        "    int* d_result;\n",
        "\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < M; j++) {\n",
        "            matrix[i][j] = 1;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaMalloc((void**)&d_matrix, N * M * sizeof(int));\n",
        "    cudaMalloc((void**)&d_result, N * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_matrix, matrix, N * M * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    sumDiagonalOfMatrix<<<N/BLOCK_SIZE/2, BLOCK_SIZE>>>(d_matrix, d_result);\n",
        "\n",
        "    cudaMemcpy(result, d_result, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaFree(d_matrix);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    for (int i = 0; i < N / BLOCK_SIZE / 2; i++) {\n",
        "        printf(\"Suma po blokovima JE %d\\n\", result[i]);\n",
        "    }\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXggT81LbM5t"
      },
      "outputs": [],
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 256 // rows\n",
        "#define M 256 // columns\n",
        "#define BLOCK_SIZE 8 // threads per block\n",
        "\n",
        "__global__ void sumDiagonalOfMatrix(int* matrix, int* result, int* globalSum) {\n",
        "    __shared__ int parcijalneSume[BLOCK_SIZE];\n",
        "\n",
        "    // Initialize partial sum for this thread\n",
        "    int parcijalSum = 0;\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    parcijalSum += matrix[(blockIdx.x * 2 * blockDim.x + tid) * N + blockIdx.x * 2 * blockDim.x + tid] +\n",
        "                   matrix[(blockIdx.x * 2 * blockDim.x + tid + BLOCK_SIZE) * N + blockIdx.x * 2 * blockDim.x + tid + BLOCK_SIZE + 1];\n",
        "\n",
        "    parcijalneSume[tid] = parcijalSum;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            parcijalneSume[tid] += parcijalneSume[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        result[blockIdx.x] = parcijalneSume[0];\n",
        "    }\n",
        "\n",
        "    // Use atomicAdd to update the globalSum with the block's result\n",
        "    if (tid == 0) {\n",
        "        atomicAdd(globalSum, result[blockIdx.x]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int matrix[N][M];\n",
        "    int result[N];\n",
        "    int* d_matrix;\n",
        "    int* d_result;\n",
        "    int* d_globalSum;\n",
        "    int globalSum = 0;\n",
        "\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < M; j++) {\n",
        "            matrix[i][j] = 1;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaMalloc((void**)&d_matrix, N * M * sizeof(int));\n",
        "    cudaMalloc((void**)&d_result, N * sizeof(int));\n",
        "    cudaMalloc((void**)&d_globalSum, sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_matrix, matrix, N * M * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Initialize globalSum on the device to 0\n",
        "    cudaMemset(d_globalSum, 0, sizeof(int));\n",
        "\n",
        "    sumDiagonalOfMatrix<<<N / BLOCK_SIZE / 2, BLOCK_SIZE>>>(d_matrix, d_result, d_globalSum);\n",
        "\n",
        "    cudaMemcpy(result, d_result, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&globalSum, d_globalSum, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(d_matrix);\n",
        "    cudaFree(d_result);\n",
        "    cudaFree(d_globalSum);\n",
        "\n",
        "    for (int i = 0; i < N / BLOCK_SIZE / 2; i++) {\n",
        "        printf(\"Suma po blokovima JE %d\\n\", result[i]);\n",
        "    }\n",
        "\n",
        "    printf(\"Global sum of all block sums: %d\\n\", globalSum);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dt5AXbDozj0",
        "outputId": "adda417f-4a34-4036-9caf-fa633eaa9c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-cg6fihsh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-cg6fihsh\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4295 sha256=d2d2dd57bb9ce2436894e1c5e99896214bb872683aadee6988e9b5bfb71889ed\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kuc55_eb/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}