{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aleksandartasic/CUDA-paralelnisistemi/blob/main/jun2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBmxp1MDnu8n",
        "outputId": "bef558d5-428a-458c-dcab-c29779734fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimal element on the main diagonal: -9223\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define SIZE 16\n",
        "#define THREADS_PER_BLOCK 2\n",
        "\n",
        "__global__ void findMinOnDiagonal(int* matrix, int* result)\n",
        "{\n",
        "    __shared__ int sharedMin[THREADS_PER_BLOCK];\n",
        "\n",
        "\n",
        "    int startIndex = threadIdx.x * SIZE/blockDim.x ; //ne radi !!!!! koliko elemenata obradjuje thread svaki                                 //SIZE/blockDim.x je razmak izmedju pocetnih elemenata blokova. start=0*(16/2)=0; end=1*(16/2)=8\n",
        "///ako ispada da thread 0 radi sa elementom prve vrste sa indeksom 0, 8, thread 1 radi sa elementom sa elementima prve vrste sa indeksima 1,9\n",
        "                                                                                      //SIZE/blockDim.x je razmak izmedju pocetnih elemenata blokova. start=0*(16/2)=0; end=1*(16/2)=8\n",
        "\n",
        "    int minVal = matrix[startIndex * SIZE + startIndex];//opseg onoga sto radi jedan thread je size/blockDim.x;\n",
        "    int i = startIndex; // < endIndex; i++)\n",
        "\n",
        "    int val = matrix[i * SIZE + i];\n",
        "    if (val < minVal)\n",
        "    minVal = val;\n",
        "\n",
        "    sharedMin[threadIdx.x] = minVal;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reduce the sharedMin array to find the overall minimum\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1)\n",
        "    {\n",
        "        if (threadIdx.x < stride)\n",
        "        {\n",
        "            if (sharedMin[threadIdx.x + stride] < sharedMin[threadIdx.x])\n",
        "                sharedMin[threadIdx.x] = sharedMin[threadIdx.x + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (threadIdx.x == 0)\n",
        "        atomicMin(result, sharedMin[0]);\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int matrix[SIZE * SIZE] = {\n",
        "        5, 2, 3, 4, 2, 9, 6, 7, 3, 6, 8, 1,5, 2, 3,4,\n",
        "        2, 9, 6, 7,3, 6, 8, 1, 4, 7, 1, 4,4, 7, 1, 4,\n",
        "        5, 2, 3, 4,2, 9, 6, 7, 3, 6, 8, 1,4, 7, 1, 4,\n",
        "        3, 6, 8,-1,5, 2, 3, 4, 2, 9, 6, 7, 3, 6, 8, 1,\n",
        "        2, 9, 6, 7,3, 6, 8, 1, 4, 7, 1, 4,4, 7, 1, 4,\n",
        "        5, 2, 3, 4,2, 9, 6, 7, 3, 6, 8, 1,4, 7, 1, 4,\n",
        "        3, 6, 8, 1,5, 2, 3, 4, 2, 9, 6, 7, 3, 6, 8, 1,\n",
        "        2, 9, 6, 7,3, 6, 8, -41555555, 4, 7, 1, 4,4, 7, 1, 4,\n",
        "        5, 2, 3, 4,2, 9, 6, -7777, -9223, 6, 8, 1,4, 7, 1, 4,\n",
        "         5, 2, 3, 4,2, 9, 6, 7, 3, 6, 8, 1,4, 7, 1, 4,\n",
        "        3, 6, 8, 1,5, 2, 3, 4, 2, 9,-6, 7, 3, 6, 8, 1,\n",
        "        2, 9, 6, 7,3, 6, 8, 1, 4, 7, 1, 4,4, 7, 1, 4,\n",
        "        5, 2, 3, 4,2, 9, 6, 7, 3, 6, 8, 1,-444, 7, 1, 4,\n",
        "        3, 6, 8, 1,5, 2, 3, 4, 2, 9, 6, 7,-311111, -4444446, 8, 1,\n",
        "        2, 9, 6, 7,3, 6, 8, 1, 4, 7, 1, 4,4, 7,5, -55554,\n",
        "        5, 2, 3, 4,2, 9, 6, 7, 3, 6, 8, 1,4, 7, 1, 4,\n",
        "\n",
        "    };\n",
        "\n",
        "    int* dev_matrix;\n",
        "    int* dev_result;\n",
        "    int result;\n",
        "\n",
        "    cudaMalloc((void**)&dev_matrix, SIZE * SIZE * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_result, sizeof(int));\n",
        "\n",
        "    cudaMemcpy(dev_matrix, matrix, SIZE * SIZE * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_result, &matrix[0], sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    findMinOnDiagonal<<<8, THREADS_PER_BLOCK>>>(dev_matrix, dev_result);\n",
        "\n",
        "    cudaMemcpy(&result, dev_result, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(dev_matrix);\n",
        "    cudaFree(dev_result);\n",
        "\n",
        "    printf(\"Minimal element on the main diagonal: %d\\n\", result);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "using namespace std;\n",
        "\n",
        "#define BLOCK_SIZE 8\n",
        "//zadatak gde se sabiraju 2 matrice i gde se u vektor sme≈°ta prosecna vrednost po koloni matrice\n",
        "template <typename T>\n",
        "__host__ void initialize_vector(T*& v, int n);\n",
        "__host__ void operate_on_GPU(float* A, float* B, float* C, float* v, int n, int m);\n",
        "__global__ void kernel_sum(float* A, float* B, float* C, int n, int m);\n",
        "__global__ void kernel_avg(float* C, float* avg, int n, int m, int el_numb);\n",
        "__host__ bool check_result(float* A, float* B, float* C, float* v, int n, int m);\n",
        "template <typename T>\n",
        "__host__ void print_vector(const char* lbl, T* A, int n);\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    float *A, *B, *C, *res;\n",
        "    int n = 221;\n",
        "\n",
        "    initialize_vector(A, n * n);\n",
        "    initialize_vector(B, n * n);\n",
        "    C = (float*) malloc(sizeof(float) * n * n);\n",
        "    res = (float*) malloc(sizeof(float) * n);\n",
        "\n",
        "    operate_on_GPU(A, B, C, res, n, n);\n",
        "\n",
        "    if(check_result(A, B, C, res, n, n))\n",
        "      cout << n << \" Correct!\" << endl;\n",
        "    else\n",
        "      cout << n << \" False!\" << endl;\n",
        "\n",
        "    free(res);\n",
        "    free(C);\n",
        "    free(B);\n",
        "    free(A);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "__host__ void initialize_vector(T*& v, int n)\n",
        "{\n",
        "    v = (T*) malloc(sizeof(T) * n);\n",
        "    for(int i = 0; i < n; i++)\n",
        "      v[i] = T(i + 1);\n",
        "}\n",
        "\n",
        "__host__ void operate_on_GPU(float* A, float* B, float* C, float* res, int n, int m)\n",
        "{\n",
        "    float* dev_A, *dev_B, *dev_C, *dev_res;\n",
        "    dim3 grid(min(256, m / BLOCK_SIZE + 1), min(256, n / BLOCK_SIZE + 1)),\n",
        "        blck(BLOCK_SIZE, BLOCK_SIZE);\n",
        "\n",
        "    size_t full = sizeof(float) * n * m,\n",
        "          part = sizeof(float) * n * m;\n",
        "\n",
        "    cudaError_t err;\n",
        "\n",
        "    err = cudaMalloc(&dev_A, full);\n",
        "    if(err)\n",
        "      cout << \"1A \" << cudaGetErrorString(err) << endl;\n",
        "\n",
        "    err = cudaMalloc(&dev_B, full);\n",
        "    if(err)\n",
        "          cout << \"1B \" << cudaGetErrorString(err) << endl;\n",
        "\n",
        "    err = cudaMalloc(&dev_C, full);\n",
        "    if(err)\n",
        "          cout << \"1C \" << cudaGetErrorString(err) << endl;\n",
        "\n",
        "    err = cudaMalloc(&dev_res, part);\n",
        "    if(err)\n",
        "          cout << \"1res \" << cudaGetErrorString(err) << endl;\n",
        "\n",
        "    err = cudaMemcpy(dev_A, A, full, cudaMemcpyHostToDevice);\n",
        "    if(err)\n",
        "          cout << \"2A \" << cudaGetErrorString(err) << endl;\n",
        "\n",
        "    err = cudaMemcpy(dev_B, B, full, cudaMemcpyHostToDevice);\n",
        "    if(err)\n",
        "          cout << \"2B \" << cudaGetErrorString(err) << endl;\n",
        "\n",
        "    kernel_sum<<<grid, blck>>>(dev_A, dev_B, dev_C, n, m);\n",
        "    kernel_avg<<<grid, blck>>>(dev_C, dev_res, n, m, n);\n",
        "    int new_n = grid.y;\n",
        "    grid.y = 1;\n",
        "    kernel_avg<<<grid, blck>>>(dev_res, dev_res, new_n, m, n);\n",
        "\n",
        "    err = cudaMemcpy(C, dev_C, full, cudaMemcpyDeviceToHost);\n",
        "    if(err)\n",
        "          cout << \"4C \" << cudaGetErrorString(err) << endl;\n",
        "\n",
        "    err = cudaMemcpy(res, dev_res, sizeof(float) * m, cudaMemcpyDeviceToHost);\n",
        "    if(err)\n",
        "          cout << \"4res \" << cudaGetErrorString(err) << endl;\n",
        "\n",
        "    cudaFree(dev_res);\n",
        "    cudaFree(dev_C);\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_A);\n",
        "}\n",
        "\n",
        "__global__ void kernel_sum(float* A, float* B, float* C, int n, int m)\n",
        "{\n",
        "    int tid_x = blockIdx.x * blockDim.x + threadIdx.x,\n",
        "      tid_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    for(int i = tid_y; i < n; i += gridDim.y * blockDim.y)\n",
        "      for(int j = tid_x; j < m; j += gridDim.x * blockDim.x)\n",
        "        C[i * m + j] = A[i * m + j] + B[i * m + j];\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void kernel_avg(float* C, float* res, int n, int m, int el_numb)\n",
        "{\n",
        "    int tid_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int tid_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    __shared__ float sh[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    // Initialize shared memory to zero\n",
        "    sh[threadIdx.y][threadIdx.x] = 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int j = tid_x; j < m; j += gridDim.x * blockDim.x)\n",
        "    {\n",
        "        for (int i = tid_y; i < n; i += gridDim.y * blockDim.y)\n",
        "        {\n",
        "            sh[threadIdx.y][threadIdx.x] += C[i * m + j];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        // Reduce the sum within the block using parallel reduction\n",
        "        for (int s = blockDim.y >> 1; s > 0; s >>= 1)\n",
        "        {\n",
        "            if (threadIdx.y < s)\n",
        "            {\n",
        "                sh[threadIdx.y][threadIdx.x] += sh[threadIdx.y + s][threadIdx.x];\n",
        "            }\n",
        "            __syncthreads();\n",
        "        }\n",
        "\n",
        "        // Store the result in the output array\n",
        "        if (threadIdx.y == 0)\n",
        "        {\n",
        "            if (gridDim.y == 1)\n",
        "            {\n",
        "                res[j] = sh[0][threadIdx.x] / (float)el_numb;\n",
        "            }\n",
        "            else\n",
        "            {\n",
        "                res[blockIdx.y * m + j] = sh[0][threadIdx.x] / (float)el_numb;\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__host__ bool check_result(float* A, float* B, float* C, float* res, int n, int m)\n",
        "{\n",
        "    bool c = true;\n",
        "\n",
        "    float* tmp = (float*) malloc(sizeof(float) * m);\n",
        "    for(int j = 0; c && j < m; j++)\n",
        "    {\n",
        "      tmp[j] = 0;\n",
        "      for(int i = 0; i < n; i++)\n",
        "      {\n",
        "        c = C[i * m + j] == (A[i * m + j] + B[i * m + j]);\n",
        "        tmp[j] += C[i * m + j];\n",
        "      }\n",
        "    }\n",
        "\n",
        "    for(int i = 0; c && i < m; i++)\n",
        "      c = res[i] == (tmp[i] / n);\n",
        "\n",
        "    free(tmp);\n",
        "    return c;\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "__host__ void print_vector(const char* lbl, T* A, int n)\n",
        "{\n",
        "    cout << lbl << \" = |\\t\";\n",
        "    for(int i = 0; i < n; (cout << A[i++] << \"\\t\"));\n",
        "    cout << \"|\\n\";\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOktN09LDWU-",
        "outputId": "31b73d61-01bb-4a38-f18e-671cc3991bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "221 False!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// CUDA kernel to calculate the minimum along the diagonal\n",
        "//radi sa razlicitim dimenzijama grida, blokova i sto je najbitnije, radi tako da jedna nit moze da radi sa vise kolona matrice.\n",
        "__global__ void diagonalMin(int* matrix, int* results, int size) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int min_val = 10000;\n",
        "\n",
        "    for (int i=0;i<size/gridDim.x/blockDim.x;i++) { ///samo to radi jedna nit\n",
        "        int element = matrix[tid * size + tid];\n",
        "        min_val = min(min_val, element);\n",
        "        tid += blockDim.x * gridDim.x+1; ///jako bitno da jedna nit zna kako da dodje do narednog elementa.\n",
        "    }\n",
        "\n",
        "    // Store the minimum value for this block in shared memory\n",
        "    __shared__ int block_min[8]; // Assuming a maximum of 256 threads per block\n",
        "    block_min[threadIdx.x] = min_val;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reduce within the block to find the block minimum\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (threadIdx.x < stride) {\n",
        "            block_min[threadIdx.x] = min(block_min[threadIdx.x], block_min[threadIdx.x + stride]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Store the block minimum in the results array\n",
        "    if (threadIdx.x == 0) {\n",
        "        results[blockIdx.x] = block_min[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = 16; // Matrix size (assumed square)\n",
        "    int matrix_size = size * size * sizeof(int);\n",
        "    //int* matrix = new int[size * size];\n",
        "    int* dev_matrix;\n",
        "    int* dev_results;\n",
        "    int num_blocks =4; // Number of CUDA blocks\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    cudaMalloc((void**)&dev_matrix, matrix_size);\n",
        "    cudaMalloc((void**)&dev_results, num_blocks * sizeof(int));\n",
        "\n",
        "    // Initialize the matrix on the host\n",
        "\n",
        "    int matrix[size*size] = {\n",
        "        5, 2, 3, 4, 2, 9, 6, 7, 3, 6, 8, 1,5, 2, 3,4,\n",
        "        2, 9, 6, 7,3, 6, 8, 1, 4, 7, 1, 4,4, 7, 1, 4,\n",
        "        5, 2, -63, 4,2, 9, 6, 7, 3, 6, 8, 1,4, 7, 1, 4,\n",
        "        3, 6, 8,-55,-6445, 2, 3, 4, 2, 9, 6, 7, 3, 6, 8, 1,\n",
        "        2, 9, 6, 7,3, 6, 8, 1, 4, 7, 1, 4,4, 7, 1, 4,\n",
        "        5, 2, 3, 4,2, 9, 6, 7, 3, 6, 8, 1,4, 7, 1, 4,\n",
        "        3, 6, 8, 1,5, 2, 3, 4, 2, 9, 6, 7, 3, 6, 8, 1,\n",
        "        2, 9, 6, 7,3, 6, 8, -5, 4, 7, 1, 4,4, 7, 1, 4,\n",
        "        5, 2, 3, 4,2, 9, 6, -7, 4, 6, 8, 1,4, 7, 1, 4,\n",
        "         5, 2, 3, 4,2, 9, 6, 7, 3, 6, 8, 1,4, 7, 1, 4,\n",
        "        3, 6, 8, 1,5, 2, 3, 4, 2, 9,55, 7, 3, 6, 8, 1,\n",
        "        2, 9, 6, 7,3, 6, 8, 1, 4, 7, 1, 4,4, 7, 1, 4,\n",
        "        5, 2, 3, 4,2, 9, 6, 7, 3, 6, 8, 1,-4, 7, 1, 4,\n",
        "        3, 6, 8, 1,5, 2, 3, 4, 2, 9, 6, 7,-3, -4656566, 8, 1,\n",
        "        2, 9, 6, 7,3, 6, 8, 1, 4, 7, 1, 4,4, 7,-54, -4,\n",
        "        5, 2, 3, 4,2, 9, 6, 7, 3, 6, 8, 1,4, 7, 1, 4,\n",
        "\n",
        "    };\n",
        "\n",
        "    // Copy the matrix from the host to the device\n",
        "    cudaMemcpy(dev_matrix, matrix, matrix_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch the CUDA kernel\n",
        "    diagonalMin<<<num_blocks, 2>>>(dev_matrix, dev_results, size);\n",
        "\n",
        "    // Copy the results back to the host\n",
        "    int* results = new int[num_blocks];\n",
        "    cudaMemcpy(results, dev_results, num_blocks * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Find the minimum among the block results\n",
        "    int min_diagonal = 100000;\n",
        "    for (int i = 0; i < num_blocks; i++) {\n",
        "        min_diagonal = min(min_diagonal, results[i]);\n",
        "    }\n",
        "\n",
        "    // Print the minimum diagonal element\n",
        "    std::cout << \"Minimum diagonal element: \" << min_diagonal << std::endl;\n",
        "\n",
        "    // Clean up\n",
        "    delete[] matrix;\n",
        "    delete[] results;\n",
        "    cudaFree(dev_matrix);\n",
        "    cudaFree(dev_results);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ632dpZzpAJ",
        "outputId": "81704df2-5e0b-463f-b0d7-2c157cc7dd32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum diagonal element: -4656566\n",
            "munmap_chunk(): invalid pointer\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORP6ihDTn1HD",
        "outputId": "bfeb4887-bc94-46ce-9739-5871a4d5f0ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-e4mn85e0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-e4mn85e0\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4295 sha256=581e17476a30e8b95524b97ad8dbafa97d10c03684ed551d71780c1469e92e8a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x_p02gm2/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    }
  ]
}